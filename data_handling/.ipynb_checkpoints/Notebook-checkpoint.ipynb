{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde649c8",
   "metadata": {},
   "source": [
    "# Data retrieval and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9e845",
   "metadata": {},
   "source": [
    "We import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057ed758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.std import tqdm\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e457311",
   "metadata": {},
   "source": [
    "## Retrieval and Storage of Basic files from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829230c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'raw_profiles'\n",
    "PLATE_NUMBER = '24277'\n",
    "LOCAL_DATA_PATH = os.path.join(os.path.expanduser('~'), \".morpho_minds_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fcc539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_structure(plate_number):\n",
    "        \"\"\"\n",
    "        Check for folder structure and create it when needed.\n",
    "        \"\"\"\n",
    "        ## Check if data folders exists. If not, create it.\n",
    "        if not os.path.exists(LOCAL_DATA_PATH):\n",
    "            os.makedirs(LOCAL_DATA_PATH)\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'raw'))\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'processed'))\n",
    "\n",
    "        if not os.path.exists(Path(LOCAL_DATA_PATH).joinpath(plate_number)):\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number))\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'raw'))\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'processed'))\n",
    "\n",
    "        if not os.path.exists(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'raw')):\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'raw'))\n",
    "\n",
    "        if not os.path.exists(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'processed')):\n",
    "            os.makedirs(Path(LOCAL_DATA_PATH).joinpath(plate_number, 'processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e852df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"\n",
    "    Download a file from GCS. Is called blob so is generic but will retrieve the SQLite DB.\n",
    "\n",
    "    :param bucket_name: The name of the bucket\n",
    "    :param source_blob_name: The name of the blob\n",
    "    :param destination_file_name: The name of the file to save the blob to\n",
    "    \"\"\"\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client()\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    # Get the blob\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    # Download the blob to a destination file\n",
    "    with open(destination_file_name, 'wb') as f:\n",
    "        with tqdm.wrapattr(f, \"write\", total=blob.size) as file_obj:\n",
    "            storage_client.download_blob_to_file(blob, file_obj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57203d0",
   "metadata": {},
   "source": [
    "## Merge of Dataframes into Useful Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ddb700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plate:\n",
    "    def __init__(self, plate_number=None, chem_df=None, images_df=None, well_df=None, plate_df=None):\n",
    "        self.plate_number = plate_number\n",
    "        self.chem_df = chem_df\n",
    "        self.images_df = images_df\n",
    "        self.well_df = well_df\n",
    "        self.plate_df = plate_df\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load the all the plate data into different dataframes.\n",
    "        \"\"\"\n",
    "        # Check for folder structure and create it when needed.\n",
    "        create_folder_structure(self.plate_number)\n",
    "\n",
    "        ## Check that file chemical_compounds.csv exists locally. If not, download it.\n",
    "        data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(self.plate_number, 'chemical_annotations.csv')\n",
    "        data_query_cached_exists = data_query_cache_path.is_file()\n",
    "\n",
    "        if data_query_cached_exists:\n",
    "            print('Loading Chemical Annotations from local CSV...')\n",
    "            chem_df = pd.read_csv(data_query_cache_path)\n",
    "        else:\n",
    "            print('Loading Chemical Annotations from remote server...')\n",
    "            download_blob(BUCKET_NAME,\n",
    "                        f'{self.plate_number}/chemical_annotations.csv',\n",
    "                        Path(LOCAL_DATA_PATH).joinpath(self.plate_number, 'chemical_annotations.csv')\n",
    "                        )\n",
    "            chem_df = pd.read_csv(data_query_cache_path)\n",
    "\n",
    "        ## Check that sqlite db exists locally. If not, download it.\n",
    "        data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(self.plate_number, 'raw', f'{self.plate_number}.sqlite')\n",
    "        data_query_cached_exists = data_query_cache_path.is_file()\n",
    "\n",
    "        if data_query_cached_exists:\n",
    "            print('Loading SQLite DB from local DB...')\n",
    "        else:\n",
    "            print('Loading SQLite DB from remote DB...')\n",
    "            download_blob(BUCKET_NAME,\n",
    "                        f'{self.plate_number}/{self.plate_number}.sqlite',\n",
    "                        Path(LOCAL_DATA_PATH).joinpath(self.plate_number, 'raw', f'{self.plate_number}.sqlite')\n",
    "                        )\n",
    "\n",
    "        conn = sqlite3.connect(data_query_cache_path)\n",
    "        query = \"\"\"\n",
    "                SELECT Image_URL_OrigAGP, Image_URL_OrigDNA, Image_URL_OrigER, Image_URL_OrigMito, Image_URL_OrigRNA, Image_Count_Cells\n",
    "                FROM Image\n",
    "                \"\"\"\n",
    "        cursor = conn.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        images_df = pd.DataFrame(data, columns=[column[0] for column in cursor.description])\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        ## Check that mean_well_profile.csv exists. If not, download it.\n",
    "        data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(self.plate_number, 'raw', 'mean_well_profiles.csv')\n",
    "        data_query_cached_exists = data_query_cache_path.is_file()\n",
    "\n",
    "        if data_query_cached_exists:\n",
    "            print('Loading Well Profiles from local CSV...')\n",
    "            well_df = pd.read_csv(data_query_cache_path)\n",
    "        else:\n",
    "            print('Loading Well Profiles from remote server...')\n",
    "            download_blob(BUCKET_NAME,\n",
    "                        f'{self.plate_number}/mean_well_profiles.csv',\n",
    "                        Path(LOCAL_DATA_PATH).joinpath(self.plate_number, 'raw', 'mean_well_profiles.csv')\n",
    "                        )\n",
    "            well_df = pd.read_csv(data_query_cache_path)\n",
    "        self.chem_df = chem_df\n",
    "        self.images_df = images_df\n",
    "        self.well_df = well_df\n",
    "        \n",
    "        print('âœ… Data loaded successfully.')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def merge_data(self):\n",
    "        \"\"\"\n",
    "        Clean the data.\n",
    "        \"\"\"\n",
    "        wells_df = self.images_df.applymap(lambda x: x.split('/')[-1].split('_')[1])\n",
    "        wells_df['well'] = wells_df.apply(lambda row: row.unique()[0] if row.nunique()==1 else 0, axis=1)\n",
    "\n",
    "        photo_number_df = self.images_df.applymap(lambda x: x.split('/')[-1].split('_')[2])\n",
    "        photo_number_df['photo_number'] = photo_number_df.apply(lambda row: row.unique()[0] if row.nunique()==1 else float('NaN'), axis=1)\n",
    "        \n",
    "        self.images_df['Image_URL_OrigAGP'] = self.images_df['Image_URL_OrigAGP'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Raw_pictures', f'{PLATE_NUMBER}-Ph_golgi', x.split('/')[-1]))\n",
    "        self.images_df['Image_URL_OrigDNA'] = self.images_df['Image_URL_OrigDNA'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Raw_pictures', f'{PLATE_NUMBER}-Hoechst', x.split('/')[-1]))\n",
    "        self.images_df['Image_URL_OrigER'] = self.images_df['Image_URL_OrigER'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Raw_pictures', f'{PLATE_NUMBER}-ERSyto', x.split('/')[-1]))\n",
    "        self.images_df['Image_URL_OrigMito'] = self.images_df['Image_URL_OrigMito'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Raw_pictures', f'{PLATE_NUMBER}-Mito', x.split('/')[-1]))\n",
    "        self.images_df['Image_URL_OrigRNA'] = self.images_df['Image_URL_OrigRNA'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Raw_pictures', f'{PLATE_NUMBER}-ERSytoBleed', x.split('/')[-1]))\n",
    "            \n",
    "        self.merged_df = pd.concat([\n",
    "            self.images_df,\n",
    "            wells_df['well'],\n",
    "            photo_number_df['photo_number'],\n",
    "        ],\n",
    "        axis = 1)\n",
    "        \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0f99691b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chemical Annotations from local CSV...\n",
      "Loading SQLite DB from local DB...\n",
      "Loading Well Profiles from local CSV...\n",
      "âœ… Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "plate = Plate('24277').load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3067eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3862f73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plate_number', 'chem_df', 'images_df', 'well_df', 'plate_df', 'merged_df'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plate.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32fc6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = plate.images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0db288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BROAD_ID in CHEM_DF == Metadata_broad_sample in WELL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54db596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "2    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda row: row.nunique() ==1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3dbe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_df = wells_df.apply(lambda row: row.unique()[0] if row.nunique()==1 else float('NaN'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8df2a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "1       /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "2       /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "3       /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "4       /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "                              ...                        \n",
       "2292    /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "2293    /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "2294    /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "2295    /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "2296    /Users/pepe/.morpho_minds_data/24277/photos/24...\n",
       "Name: Image_URL_OrigRNA, Length: 2297, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df['Image_URL_OrigAGP'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Pictures', f'{PLATE_NUMBER}-Ph_golgi', x.split('/')[-1]))\n",
    "images_df['Image_URL_OrigDNA'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Pictures', f'{PLATE_NUMBER}-Hoechst', x.split('/')[-1]))\n",
    "images_df['Image_URL_OrigER'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Pictures', f'{PLATE_NUMBER}-ERSyto', x.split('/')[-1]))\n",
    "images_df['Image_URL_OrigMito'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Pictures', f'{PLATE_NUMBER}-Mito', x.split('/')[-1]))\n",
    "images_df['Image_URL_OrigRNA'].apply(lambda x: Path(LOCAL_DATA_PATH).joinpath(PLATE_NUMBER, 'Pictures', f'{PLATE_NUMBER}-ERSytoBleed', x.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcff5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGP == Ph-golgi\n",
    "# DNA == Hoechst\n",
    "# ER == ERSyto\n",
    "# Mito == Mito\n",
    "# RNA == ERSytoBleed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2db0d8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image_URL_OrigAGP',\n",
       " 'Image_URL_OrigDNA',\n",
       " 'Image_URL_OrigER',\n",
       " 'Image_URL_OrigMito',\n",
       " 'Image_URL_OrigRNA']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bdce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
